---
layout: post
title:  ATOMO Communication-efficient Learning via Atomic Sparsification
date:   2020-11-13
author:     Leo
header-img: img/post_bao_yan.jpg
catalog: true
tags:
    - Sparse Communication
---
<head>
<script>
MathJax = {
tex: {
inlineMath: [['$', '$']],
displayMath: [['\\[', '\\]']],
processEnvironments: true,
processRefs: true
},
options: {
skipHtmlTags: ['noscript', 'style', 'textarea', 'pre', 'code'],
ignoreHtmlClass: 'tex2jax_ignore',
renderActions: {
find_script_mathtex: [10, function (doc) {
for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
const display = !!node.type.match(/; *mode=display/);
const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
const text = document.createTextNode('');
node.parentNode.replaceChild(text, node);
math.start = { node: text, delim: '', n: 0 };
math.end = { node: text, delim: '', n: 0 };
doc.math.push(math);
}
}, '']
}
},
svg: {
fontCache: 'global'
}
};
</script>
<script id="MathJax-script" async src="https://cdn.staticfile.org/mathjax/3.0.1/es5/tex-svg.js"></script>
</head>

## 前言

 诸如TensorFlow、MXNet和Caffe2的几个机器学习框架中的分布式训练方法得到的加速增益通常达不到人们的预期。通信开销是这种加速饱和现象的来源。

 通信瓶颈很大程度上来源于在通信节点之间传输的连续梯度更新。随着模型参数的增加，梯度的大小会成指数的增加。这些瓶颈在联邦学习中更加显著（pronounced）。在联邦学习中，边缘设备，如手机，传感器等，也参与分布式训练，但是它们在向上传输信息（up-link）时，存在着低带宽的缺点。

为了减少分布式训练中的通信代价，最近的一系列研究提出了通信较低精度的梯度或是梯度的稀疏版本。

**本文贡献**：原子分解（atomic decomposition）将向量表示为内积空间中简单构造单元的线性组合。本文的工作证明了随机梯度稀疏化和量化是在任何可能的原子分解中稀疏梯度的通用方法的各个方面，这些分解包括了逐个元素或奇异值分解，傅立叶分解等。考虑到这一点，本文提出了ATOMO，这是用于随机梯度原子稀疏化的通用框架。 ATOMO设置并以最佳方式求解最小化稀疏梯度的方差的元优化问题，约束条件为他在原子基上是稀疏的，并且仍然是输入的无偏估计。

本文证明，1-bit QSGD和TernGrad是ATOMO的特殊情况，并且在不同的参数范围内，他们都是最优的（考虑方差和稀疏度）。接着，本文认为对于一些神经网络应用，将梯度视作是矩阵的级联（每一个对应一个层），并且将原子稀疏化应用于他们的奇异值分解（SVD）中是有意义的。由于这些矩阵近似是低秩的，因此具有良好的动机。本文展示了在与QSGD和TernGrad相同的通信预算下，将ATOMO应用于每个层的奇异值分解中，可以带来更小的方差和更快的训练。本文使用扩展实验展示将ATOMO和奇异值分解稀疏化结合，可以带来与QSGD或TernGrad相比2/3倍的训练速度提升。这适用于在SVHN和CIFAR-10数据集上使用VGG和ResNet-18网络结构。

**与前人工作的关系**：ATOMO与通信高效的分布式均值估计文章中的工作相近。这些工作都提到，方差（或者说是均方误差）控制了算法的重要性质，例如收敛性质，并且他们试图找出一个低通信的向量平均结构，去缩减方差。本文工作与他们主要有两点区别：

1. 本文对于所有的输入梯度，推导出了方差最小化问题的封闭解。
2. ATOMO适用于任意原子分解，这使得我们可以比较矩阵的逐个元素稀疏和奇异值稀疏。这样，本文推导出了使用相同的稀疏度预算，SVD稀疏带来更小的方差的精确条件。

## 问题设定

机器学习中的经验风险（empirical risk）定义为：
\[
f(\omega)=\frac{1}{n}\sum_{i=1}^{n}\ell (\omega;x_i)
\]
其中$x_i\in\mathbb{R}^d$是第$i$个数据点。随机梯度方法是近似最小化$f(\omega)$的其中一个方法，该操作为：
$$
\omega_{k+1}=\omega_{k}-\gamma \hat{g}(\omega_{k})
$$

其中$\omega_{0} $是初始模型，$\gamma$是步长，$\hat{g}(\omega)$是$f(\omega)$的一个随机梯度，即它是真实梯度$g(\omega)=\nabla f(\omega)$的一个无偏估计。

为了证明基于随机梯度方法的收敛性，我们经常需要$\hat{g}(\omega)$是全局梯度的一个无偏估计，并且具有较小的方差$\mathbb{E}\Vert\hat{g}(\omega)\Vert^2$，因为它控制了收敛的速度。具体来说，假设$\omega^*$是$f$的最优解，那么我们有
$$
\mathbb{E}[\Vert \omega_{k+1}-\omega^*\Vert_2^2]=\mathbb{E}[\Vert \omega_{k}-\omega^*\Vert_2^2]-(2\gamma\langle \nabla f(\omega_k),\omega_k-\omega^*\rangle-\gamma^2\mathbb{E}\Vert\hat{g}(\omega)\Vert_2^2)
$$

右边式子第二项中，$\mathbb{E}\Vert\hat{g}(\omega)\Vert^2$越小，该项越大，收敛速度越快。

既然方差代表了收敛的速度，在通信高效的随机梯度方法中，有人会问：用一个$k$比特表示的随机梯度的最小方差是多少？这可以被建模成一个元优化问题：
$$
\min_g \mathbb{E}\Vert\hat{g}(\omega)\Vert^2
$$

$$
s.t.\quad \mathbb{E}[\hat{g}(\omega)]=g(\omega), \quad \hat{g}(\omega) \text{ can be expressed with k bits}
$$

这里，期望作用在$\hat{g}$的随机性上。我们对设计一个随机估计$\hat{g}$来“求解”该优化问题感兴趣。但是，最后一个约束的形式化，易于处理的形式似乎很难设计。这里我们将其替换为一个更简单的约束，该约束要求$\hat{g}(\omega)$对于一个给定的原子分解是稀疏的。

## ATOMO：原子分解与稀疏化

一个原子分解的例子是逐个元素分解$g=\sum_i g_ie_i$，其中$\{e_i\}_{i=1}^n$是标准基。更一般的，内积空间$V$中的任意标准正交基可以对$g$唯一分解。当关注有限维向量时，在此框架中也可以使用傅立叶分解和小波分解。当考虑矩阵时，秩为1的矩阵集可以使用奇异值分解。

本文对使用更少的原子找到$g$的近似值感兴趣。初始动机是通过只需要发送原子和非零权重来减少通信成本。我们可以使用最适合稀疏化的分解方法。例如，如果$X$是一个低秩矩阵，那么它的奇异值分解自然是稀疏的，因此我们可以通过稀疏化它的奇异值分解而不是它的元素来节约通信成本。

假设$\mathcal{A}=\{a_i\}_{i=1}^n$并且我们有一个原子分解
$g=\sum_{i=1}^n\lambda_i a_i$。我们希望找到一个$g$的无偏估计$\hat{g}$，使得它在这些原子下是稀疏的并且方差很小。由于$\hat{g}$是无偏的，最小化它的方差相当于最小化$\mathbb{E}[\Vert\hat{g}\Vert^2]$。我们使用以下估计器：
$$\hat{g}=\sum_{i=1}^n\frac{\lambda_it_i}{p_i}a_i$$

其中$t_i\sim \text{Bernoulli}(p_i)$，$0<p_i\leq 1$。我们称这个稀疏结构为*原子稀疏*。请注意，$t_i$是相互独立的，并且我们假设$\Vert a_i\Vert^2=1$。

引理1：$\mathbb{E}[\hat{g}]=g$且$\mathbb{E}[\Vert\hat{g}\Vert^2]=\sum_{i=1}^n\lambda_i^2p_i^{-1}+\sum_{i\neq j}\lambda_i\lambda_j\langle a_i,a_j\rangle$。

令$\lambda=[\lambda_1,...,\lambda_n]^T, p=[p_1,...,p_n]^T$。为了确保该估计是稀疏的，我们固定某个稀疏预算$s$，使得$\sum_i p_i=s$。这是一个*平均稀疏度*约束。我们希望在该约束条件下最小化$\mathbb{E}[\Vert\hat{g}\Vert^2]$。由引理1，该问题等价于
$$
\min_p \sum_{i=1}^n\frac{\lambda_i^2}{p_i}\quad s.t.\forall i,0<p_i\leq 1,\sum_{i=1}^{n}p_i=s.
$$

![img](https://github.com/derek1998/derek1998.github.io/blob/main/figures/ATOMO%20probabilities.png?raw=true)
